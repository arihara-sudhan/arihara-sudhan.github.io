SOURCE: NOT TO BE DISCLOSED FOR NOW
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[31/03/2025]
1. Using Attention, We can make the word vector better incorporate the context
 Example: My name is Ari
 	Here, "name" and "Ari" would have high attention score than "My" and "name"
2. Attending on The Same Sentence is called Self Attention
3. The word piece is fed to Encoder Part, which spits out More Context Aware Vector
4. Single Word that is input to the transformer would have three vectors
	- Query Vector of dq: What I am looking for
	- Key Vector of dk: What I can offer
	- Value Vector of dv: What I actually offer
5. Key and Query gets multiplied and scaled by (1/SQROOT(dk))
6. This scaling is to, stabilize the values
	- When we multiply Key and Query, the resulting value will be much higher
	- We normalize it by scaling by (1/SQROOT(dk))
7. Masking
	Padding Mask: The padded regions shouldn't be masked
	Causal (Look-ahead Mask): Future words are masked
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[31/03/2025]
8. LLM FineTuning: LLM (Large Language Model) fine-tuning is the process of adapting a pre-trained language model to perform better on a specific task or domain
9. FineTuning: Training the model on a smaller, specialized dataset while preserving the general knowledge it learned during its initial training
10. FineTuning Steps
	Collect Data → Chat logs, FAQs, Q&A pairs  
	Preprocess → Clean, format (JSON/CSV)  
	FineTune → Train using OpenAI/Hugging Face  
	Deploy → Launch & improve with feedback
11. RLHF Training: Using human feedback to make the model generate more useful, aligned, and safe responses
12. RLHF Steps
	PreTrain → Start with a general LLM
	Collect Feedback → Humans rank AI responses
	Train Reward Model → Learn from rankings
	FineTune LLM → Use reinforcement learning (PPO)
	Deploy & Improve → Iterate with more feedback
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[31/03/2025]
13. LLM Deployment
	Choose Platform → Cloud, API, or On-Premise
	Optimize Model → Convert, quantize, reduce size
	Set Up Infra → Use GPUs/TPUs, Docker, Kubernetes
	Expose API → Deploy via FastAPI, Flask, or WebSocket
	Monitor & Scale → Track, optimize, and improve
14. Cloud Platforms: AWS, GCP, Azure
15. Compute Services: EC2 (AWS), Vertex AI (GCP), Azure ML
16. Compute Services include,
	- Provision Compute Resources → Allocate CPUs, GPUs, TPUs for training/inference
	- Run AI Workloads → Execute model training, fineTuning, inference tasks
	- Manage Scalability → AutoScale based on demand
	- Optimize Performance → Load balancing, caching, acceleration
	- Provide APIs & Endpoints → Serve models as REST APIs for integration
	- Security & Monitoring → Control access, track logs, ensure uptime
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[31/03/2025]
17. Quantization: Technique used to reduce the size and computational requirements of AI models by representing weights and activations with lower precision
Types of Quantization:
	- Post-Training Quantization (PTQ): Applied after training the model.
	- Quantization-Aware Training (QAT): Quantization is integrated into training to allow the model to adapt to lower precision
18. Examples: FP32 to INT8 
19. FlashAttention: Fast and memory efficient attention mechanism
20. FlashAttention restructures how attention is computed
	1. Fused Attention
	Instead of executing multiple separate operations (matrix multiplication, softmax, dropout) across different memory locations, FlashAttention combines them into a single GPU operation
	2. I/O-Aware Memory Optimization
	Uses SRAM (fast cache memory inside GPUs) instead of slow global memory
	3. Chunked Attention Computation
	Instead of computing attention for the entire sequence at once, it processes small chunks of tokens at a time
21. vLLM: Virtual LLM
	- Traditional LLM serving frameworks waste memory when handling multiple requests
	- vLLM's PagedAttention dynamically allocates memory, reducing waste and allowing more queries to be processed simultaneously
22. MultiGPU Inference
	Using vLLM
	- python -m vllm.entrypoints.openai.api_server \ --model meta-llama/Llama-2-7b-chat-hf \ --tensor-parallel-size 2
	Using DeepSpeed (Microsoft)
	- deepspeed --num_gpus=4 train.py --deepspeed ds_config.json
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[01/04/2025]
23. Training using DeepSpeed (Optimizes training for large model)
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
import deepspeed
model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
training_args = TrainingArguments(
    output_dir="./gpt2_deepspeed",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    evaluation_strategy="steps",
    save_strategy="steps",
    save_steps=1000,
    logging_steps=500,
    learning_rate=5e-5,
    fp16=True,
    deepspeed="ds_config.json"  # Specify DeepSpeed config file
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)
trainer.train()
24. DeepSpeed Configuration (dsconfig.json)
{
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu"
    }
  },
  "fp16": {
    "enabled": true
  }
}
25. Ray Serve: Deploys scalable inference across multiple GPUs/nodes
import ray
from ray import serve
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
ray.init()
@serve.deployment(num_replicas=2, ray_actor_options={"num_gpus": 1})
class GPT2Inference:
    def __init__(self):
        self.model = AutoModelForCausalLM.from_pretrained("gpt2").to("cuda")
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2")

    def __call__(self, request):
        prompt = request.json()["prompt"]
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        outputs = self.model.generate(**inputs, max_length=50)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
gpt2_service = GPT2Inference.bind()
serve.run(gpt2_service)
26. The @serve.deployment decorator is used in Ray Serve to define how a model or function should be deployed for scalable inference
27. num_replicas=2
	- This creates two replicas of the deployment
	- Each replica is a separate instance of the GPT2Inference class
	- More replicas help in handling higher traffic loads by distributing requests across multiple instances
28. ray_actor_options={"num_gpus": 1}
	- Assigns 1 GPU per replica
	- Since num_replicas=2, and each replica gets 1 GPU, this means the system requires at least 2 GPUs to run optimally
	- Ensures that each model instance runs on a separate GPU to maximize parallel processing
29. Large Scale Experimentation (Distributed Hyperparameter Tuning with Ray Tune)
import ray
from ray import tune
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
def train_model(config):
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    args = TrainingArguments(
        output_dir="./results",
        per_device_train_batch_size=config["batch_size"],
        learning_rate=config["lr"],
        num_train_epochs=3,
        evaluation_strategy="steps",
    )
    trainer = Trainer(model=model, args=args, train_dataset=dataset)
    trainer.train()
    tune.report(loss=trainer.state.log_history[-1]["loss"])
search_space = {
    "lr": tune.grid_search([1e-5, 5e-5, 1e-4]),
    "batch_size": tune.choice([4, 8, 16])
}
ray.init()
tuner = tune.Tuner(train_model, param_space=search_space)
tuner.fit()
30. Experiment Tracking with MLFlow
import mlflow
mlflow.start_run()
mlflow.log_param("learning_rate", 5e-5)
mlflow.log_param("batch_size", 8)
mlflow.log_metric("accuracy", 0.85)
mlflow.end_run()
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[01/04/2025]
31. ONNX: Open Neural Network Exchange
32. ONNX allows you to move models seamlessly between different machine learning frameworks
	> torch.onnx.export(), tf2onnx
33. We can train a model in PyTorch, convert it to ONNX format, and then deploy it with TensorFlow or ONNX Runtime
34. ONNX is designed to improve inference performance
35. How ONNX Optimizes Models
	- Operator Fusion: Combines multiple ops into a single, efficient operation
	- Constant Folding: Precomputes constants to avoid redundant calculations
	- Graph Simplification: Removes unnecessary nodes
	- Quantization: Converts FP32 weights to INT8 for reduced memory and faster inference
36. How ONNX Ensures Interoperability
	- Graph-Based Format: Converts models to a graph with nodes (operations) and edges (tensors)
	- Universal Operators: Defines standard operations (e.g., Conv, ReLU) for all frameworks
	- Opset Versioning: Maintains compatibility across versions and frameworks
	- ONNX Runtime: Runs models on any backend (CPU, GPU, etc.) via ONNX Runtime
37. Zero Redundancy Optimizing for Data Parellism
	ZeRO Stage 1 – Optimizer State Partitioning
		Splits optimizer states across GPUs.
		Memory savings: ~2x.
		Trade-off: Slight communication overhead.
	ZeRO Stage 2 – Gradient + Optimizer Partitioning
		Splits gradients + optimizer states across GPUs.
		Memory savings: ~4x.
		Trade-off: More communication overhead.
	ZeRO Stage 3 – Full Model Partitioning
		Splits parameters + gradients + optimizer states across GPUs.
		Memory savings: 8x+.
		Trade-off: Maximum communication overhead.
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[02/04/2025]
38. Alignment techniques: Reward Models & Preference Modeling
39. A reward model is a way to give feedback to an AI system to guide its behavior. The idea is that when the AI produces an action or response, you assign a "reward" (a positive or negative score) based on how good or bad that action is
40. Preference modeling is like showing the AI which option is preferred based on human choices
41. Pruning: Pruning reduces the number of parameters in a model by eliminating weights that are near zero (unimportant parameters)
42. Two Types of Pruning
	- Weight Pruning: Remove small weights from the model
	- Neuron Pruning: Eliminate entire neurons or layers
43. Model Parallelism
	- Useful for large models, where the computation is split by layer (Pipeline Parallelism) or tensor (Tensor Parallelism)
44. Here are ultra-precise points on **Cost-efficient Training Strategies**:
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[02/04/2025]
45. Spot Instances
	- Up to 90% savings over on-demand instances
	- Checkpointing: Save model states to resume after termination
	- Fault Tolerance: Use auto-scaling or retry mechanisms for uninterrupted training
	- Hybrid Usage: Combine spot with on-demand instances for resiliency
	- Tools: AWS EC2 Spot, GCP Preemptible VMs, Azure Low-Priority VMs
46. Mixed Precision Training
	- Lower Memory Usage: 16-bit precision reduces memory footprint
	- Faster Training: 16-bit operations speed up computation
	- Cost Savings: Use smaller GPUs for training large models
	- AMP: Enable Automatic Mixed Precision in PyTorch (`torch.cuda.amp`) or TensorFlow (`tf.keras.mixed_precision`)
47. Combined Strategy
	- Use Spot Instances for cost savings
	- Enable Mixed Precision to reduce training time and memory usage
48. Cluster Types
	1. Load-Balancing Cluster: Distributes incoming requests across multiple nodes to balance the load (e.g., web servers)
	2. High-Performance Computing (HPC) Cluster: Performs parallel computations for complex tasks like simulations (scientific calculations)
	3. High Availability (HA) Cluster: Ensures continuous operation by failing over to healthy nodes if one fails (database services)
	4. Storage Cluster: Distributes and replicates data across multiple nodes for scalability and fault tolerance (cloud storage).
	5. GPU Cluster: Uses multiple GPUs to accelerate computation for tasks like deep learning training and inference
