SOURCE: Build a Large Language Model (From Scratch) - A Book by Sebastian Raschka
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[08/07/2025]
1. Large Language Models (LLMs)
- Deep neural networks trained on massive text datasets
- Understand, generate, and interpret human-like text
- Superior to older models in context-heavy tasks
2. Before vs After LLMs
- Traditional NLP models: good for classification, poor for generation
- LLMs handle complex tasks like instruction parsing and text creation
3. "Understanding" in LLMs
- LLMs simulate understanding via pattern-based generation
- No true human-like consciousness or comprehension
4. Training Method
- Trained using deep learning on vast unstructured text
- Learns context, structure, and relationships in language
5. General vs Specific Models
- Traditional models: task-specific (e.g., translation only)
- LLMs: general-purpose, capable of handling diverse NLP tasks
6. Why LLMs Perform Well
- Built on transformer architecture
- Trained on massive datasets to capture complex language patterns
7. Transformer Architecture
- Consists of encoder and decoder components
- Encoder encodes input into context-rich embeddings
- Decoder generates output from embeddings
- Uses self-attention to capture token relationships
8. BERT vs GPT
- BERT: Encoder-only, for masked word prediction and classification
- GPT: Decoder-only, for text generation and completion
9. Training Phases
- Pretraining: Self-supervised next-word prediction on unlabeled data
- Fine-tuning: Supervised learning on labeled task-specific data
10. Fine-Tuning Types
- Instruction fine-tuning: Input-output instruction pairs
- Classification fine-tuning: Input and class label pairs
11. Capabilities of LLMs
- Text generation, summarization, translation, question answering
- Enable chatbots, virtual assistants, search augmentation
12. Self-Supervised Learning
- Labels are generated from input data (e.g., next token prediction)
- Manual labels not needed for pretraining
13. Zero-shot and Few-shot Learning
- Zero-shot: Performs tasks with no examples
- Few-shot: Learns tasks from a few input examples
14. Transformers vs LLMs
- Most LLMs use transformers
- Transformers also used in other domains (e.g., vision)
- Some LLMs use RNN or CNN for better efficiency
15. Pretraining Datasets
- Use diverse and massive corpora with billions of tokens
- Include books, websites, Wikipedia, and more
- Capture syntax, semantics, and general knowledge
16. Token Definition
- Tokens are units of text (words, punctuation, etc.)
- Used as the input unit for LLMs
17. GPT-3 Dataset Details
- CommonCrawl: 410B tokens (60%)
- WebText2: 19B (22%)
- Books1 & Books2: 12B + 55B (8% + 8%)
- Wikipedia: 3B (3%)
- Trained on 300B tokens out of total 499B
18. Storage and Cost
- CommonCrawl alone ≈ 570 GB
- GPT-3 training cost ≈ $4.6 million
- Other datasets include Arxiv, StackExchange
19. Open Datasets
- Public dataset alternatives exist, e.g., Dolma (3 trillion tokens)
- May include copyrighted content; usage depends on local laws
20. Pretrained Model Use
- Many pretrained LLMs are open source
- Can be fine-tuned on small datasets for specific tasks
- Reduces training cost and time
21. GPT Architecture
- Introduced in “Improving Language Understanding by Generative Pre-Training”
- GPT-3: scaled version of original GPT
- ChatGPT based on InstructGPT fine-tuned from GPT-3
22. Next-Word Prediction
- Self-supervised learning using the next word as label
- Enables use of unlabeled text data at massive scale
23. GPT Model Structure
- Decoder-only model (no encoder)
- Generates text one token at a time
- Autoregressive: output of one step is input to next
24. Scale of GPT-3
- 96 transformer layers
- 175 billion parameters
- Much larger than original transformer (6 layers)
25. Emergent Behavior
- LLMs learn tasks like translation without explicit training
- Abilities emerge from training on large, diverse datasets
26. Model Versatility
- GPTs can complete, classify, translate, summarize without specific tuning
- General-purpose via pretraining alone
27. Stages of Building an LLM
- Stage 1: Data preparation, attention mechanism
- Stage 2: Architecture coding, pretraining, evaluation
- Stage 3: Load weights, fine-tune for tasks
28. Attention Mechanism
- Central to transformer and LLMs
- Enables selective focus across input sequence
29. Decoder-Only LLMs
- GPT-style models simplify to decoder blocks
- Effective for text generation and instruction following
30. Efficiency of Fine-Tuning
- Pretrained foundation models reduce effort for downstream tasks
- Small datasets sufficient for instruction and classification fine-tuning
31. Summary of LLM Characteristics
- Shift from rule-based to deep learning methods
- Trained by next-word prediction followed by fine-tuning
- Based on transformer’s attention mechanism
- Decoder-only architecture for text generation
- Massive datasets required for strong general capabilities
- Emergent behaviors observed without direct supervision
