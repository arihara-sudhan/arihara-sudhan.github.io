[{
		"title": "Jagged Array with Mr.Bean ",
		"logo": "https://media4.giphy.com/media/73E4wQO3OUZPO/200w.gif?cid=82a1493bezs0tirfoorlx9tbhfeu3lfn3x7yrs0o3vrj3seq&rid=200w.gif&ct=g",
		"topic": "Jagged Array with Mr.Bean",
		"num": "0",
		"feeds": ["23-04-2023", "<span>For a single like me, watching Mr.Bean is an essential part of life. In one episode, Bean can‚Äôt sleep as usual. There is no Mosquito-Problem but Cat Problem. He gets rid of it by disguising himself as a Dog and barking to deceive the cats. He knows how to solve a problem. (Whether to apply a stack, or a heap , or socks)</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*42n_Pz3cccW_sWIH5oGC6Q.jpeg\" width=\"95%\"></center>\n<span>Then, he tries to sleep but he can‚Äôt. So, he starts counting sheep in a photo. He gets messed with the count. So, to calculate the total number, he uses a calculator and computed row multiplied by the column considering the sheep as an n dimensional array. The problem here is, the column size varies from row to row.</span><br>\n<center><div class=\"iframe\"><iframe width=\"100%\" height=\"100%\" src=\"https://www.youtube.com/embed/FmbmNp1RDCE?controls=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></div></center>\n<span>He doesn‚Äôt care of it. He simply computes the RxC and sleeps. ( Somehow, he slept and the END ) It remembered me the so-called Jagged Array / Ragged Array.So, What is it ? Jagged arrays are the arrays with variable column size. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*YXl3u6SjRrzzj0QcjNBMFw.png\" width=\"95%\"></center>\n<span>If we have an array of arrays, we usually work with a fixed size of column. But, there are some situations when we don‚Äôt have fixed column size. This is the so called Jagged Array structure. Let‚Äôs declare a ragged array in Java. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*R7HbbriaoF0DjxUbHOR-dA.png\" width=\"95%\"></center>\n<span>As you can see, we declare the array to have 3 rows and a non-fixed number of columns. Now, let‚Äôs say what are the column sizes in different rows. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*yER6Prw2DbHI8UzeitlbJA.png\" width=\"95%\"></center>\n<span>So, in first row, there will be 5 elements. And the second row would have 3 and the third would have 2.Now, let‚Äôs initialize them using for-loops.</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*MXSgGARZU6qpErstshrHfQ.png\" width=\"95%\"></center>\n<span>Now, let‚Äôs pick the elements of the ragged guy dynamically. It can be used to store rows of data of varying lengths to improve performance when working with multi-dimensional arrays.</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*RBcREM1LDs_ZzTH6TG3Uig.png\" width=\"95%\"></center>\n<span>USES : It can be used to store rows of data of varying lengths to improve performance when working with multi-dimensional arrays. </span><br>\n"]
	},
	{
		"title": "Neural Network ",
		"logo": "https://media3.giphy.com/media/xT9IgN8YKRhByRBzMI/giphy.gif?cid=6c09b952fssombz2mzcueme48zefw99n75u6ifr1wewupjvl&ep=v1_internal_gif_by_id&rid=giphy.gif&ct=g",
		"topic": "Neural Network ",
		"num": "1",
		"feeds": ["28-07-2023", "<span>Neural Network is indeed one of the most remarkable discoveries in human history. It operates in a fascinating and almost magical manner. Just like a baby learns from its experiences, a neural network learns from data. But what does learning mean in the context of a neural network? Let's explore this concept in simple terms.</span>\n<span>In the Southern Boy's 3D Animation, he illustrates how a neural network works, making the learning process more accessible. The golden spheres represent neurons, which are the building blocks of a neural network.</span><br>\n<div class=\"iframe\"><iframe width=\"100%\" height=\"100%\" src=\"https://www.youtube.com/embed/IwTxj6X36w8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></div><br>\n<span>Learning in a neural network occurs through a process called \"training.\" During training, the network is exposed to a large amount of input data along with the corresponding correct output or label. The network's task is to learn patterns and relationships within the data so that it can accurately map inputs to the correct outputs.</span><br>\n<h3>USE-CASE : XOR</h3><hr>\n<span>Brace yourself for an enchanting adventure into the realm of neural networks, where intelligence and intuition converge in the pursuit of understanding this fascinating phenomenon. We all know what is XOR Operation. It gives 1 when inputs are different. It gives 0 when inputs are same.</span><br>\n<h3>LAYERS</h3><hr>\n<span>A Neural Network usually has three layers of Input Layer, Hidden Layer and Output Layer. There can be more hidden layers. They can be increased to a large extent for a subtle performance but it can also lead to some potential risks.</span><br>\n<h3>WEIGHTS AND BIASES : RANDOMNESS</h3><hr>\n<span>Imagine we are trying to tune our FM radio to find a specific channel (\"FM Rainbow\"). However, we have no idea where the exact frequency of this channel is, so we start with the radio knobs set to random positions. In the world of neural networks, similar to our radio tuning, we begin with random weights and biases. These weights and biases are like the radio knobs, but instead of controlling the radio frequency, they control the behavior of the neural network. So, initially, these weights and biases are randomly initialized.</span><br>\n<h3>FORWARD PROPAGATION</h3><hr>\n<span>Now, we turn on the radio and start scanning through different frequencies. As we twist the knobs randomly, we might not immediately land on the FM Rainbow channel. But every time we try a new random setting, we pay attention to the sound we hear. Gradually, we start adjusting the knobs based on the sound, moving closer and closer to the FM Rainbow channel. In a neural network, we do something similar during the learning process. The network starts with random weights and biases, and it takes in some input data.</span><br>\n<span>Each input is multiplied by its corresponding weight. These weights represent the strengths of connections between the inputs and the neuron. The multiplication of inputs and weights determines how much influence each input has on the neuron's output. The results of the multiplications (inputs multiplied by weights) are then added together to form a single value. In essence, the inputs are individually adjusted by their corresponding weights and then combined to create a single value, which serves as the input for the activation function.\nThe activation function in a neural network is like a switch that decides whether a neuron should be \"activated\" or \"turned on\" based on the input it receives. Just like how a light switch can either turn on the light or keep it off, the activation function determines if a neuron should fire or remain silent. When the input to a neuron passes through the activation function, it transforms the input in a certain way. If the transformed value is above a certain threshold, the neuron becomes active and sends its output to the next layer of the neural network. On the other hand, if the transformed value is below the threshold, the neuron stays inactive, and its output is not propagated further. Here, in our illustration, we use sigmoid activation function. There is a plethora of activation functions. This non-linear behavior introduced by the activation function allows neural networks to learn and capture complex patterns in the data. It's like introducing flexibility in decision-making, as neurons can either contribute significantly to the final output or not at all, depending on the transformed value.</span><br>\n<h3>LOSS</h3><hr>\n<span>Just like we kept tweaking the radio knobs and listened to the sound, the neural network does something similar with its predictions. It compares its predictions with the actual correct answers (labels) for the input data and measures how far off it was. This difference is called the \"error\" or \"loss.\" For XOR Operation, when inputs are 0 and 1, the output must be 1. But, what we get here is not 1. So, we measure the distance between the actual value and the predicted value to take a appropriate action. Here, Mean Squared Error is used. There is also a plethora of loss functions.</span><br>\n<h3>THE MAGIC</h3><hr>\n<span>The magic happens when the neural network uses a clever algorithm called \"backpropagation\" to adjust those random weights and biases based on the error it calculated. By doing this over and over again with different data, the network learns from its mistakes and gradually gets better at making predictions, just like we got closer to the FM Rainbow channel by fine-tuning the knobs.</span> <br>\n<pre style=\"color: #16FF00;\">\n1. Input Layer Neurons (x1=1, x2=0)\n   - Neuron x1: The input value is 1.\n   - Neuron x2: The input value is 0.\n\n2. Hidden Layer Neuron (h):\n   - Input: \n     - Neuron x1 sends its output (1)\n       through weight w1 (0.3).\n     - Neuron x2 sends its output (0)\n       through weight w2 (0.5).\n     - Adding bias b1 (0.1).\n     - Calculate the weighted sum:\n       z1 = (1 * 0.3) + (0 * 0.5) + 0.1\n          = 0.4\n   - Activation:\n     - The sigmoid activation function:\n       h = 1 / (1 + exp(-z1))\n       ‚âà 0.59874\n\n3. Output Layer Neuron (o):\n   - Input: \n     - Neuron h sends its output (0.59874)\n       through weight w3 (0.2).\n     - Neuron h sends its output (0.59874)\n       through weight w4 (0.4).\n     - Adding bias b2 (0.6).\n     - Calculate the weighted sum:\n       z2 = (0.59874*0.2)+(0.59874*0.4)+0.6\n        ‚âà 0.959244\n   - Activation:\n     - The sigmoid activation function:\n       o = 1 / (1 + exp(-z2)) ‚âà 0.7222\n\n4. Loss Calculation:\n   - Given that the target output for XOR with\n     inputs (1, 0) is 1, the actual output is\n     approximately 0.7222.\n   - Calculate the Mean Squared Error loss:\n     L = (1/2) * (o - y_true)^2\n       ‚âà (1/2) * (0.7222 - 1)^2\n       ‚âà 0.03858\n\n5. Backpropagation:\n   - Derivative of the loss with respect to\n     the output layer input (z2):\n     ‚àÇL/‚àÇz2 = (o - y_true) * o * (1 - o)\n     ‚âà (0.7222 - 1) * 0.7222 * (1 - 0.7222)\n     ‚âà -0.14719\n\n   - Derivative of the loss with respect to\n     the hidden layer output (h):\n     ‚àÇL/‚àÇh = ‚àÇL/‚àÇz2 * w3\n     ‚âà -0.14719 * 0.2 ‚âà -0.02944\n\n   - Derivative of the loss with respect to\n     the input layer weights and bias:\n     ‚àÇL/‚àÇw1 = ‚àÇL/‚àÇh * h * (1 - h) * x1\n     ‚âà -0.02944*0.59874*(1 - 0.59874)*1\n     ‚âà -0.00783\n     ‚àÇL/‚àÇw2 = ‚àÇL/‚àÇh * h * (1 - h) * x2\n     ‚âà -0.02944 * 0.59874*(1 - 0.59874)*0\n     ‚âà 0\n     ‚àÇL/‚àÇb1 = ‚àÇL/‚àÇh * h * (1 - h)\n     ‚âà -0.02944*0.59874 * (1 - 0.59874)\n     ‚âà -0.01108\n\n   - Derivative of the loss w.r.to the o/p\n     layer weights and bias:\n     ‚àÇL/‚àÇw3 = ‚àÇL/‚àÇz2 * h\n     ‚âà -0.14719 * 0.59874\n     ‚âà -0.08812\n     ‚àÇL/‚àÇw4 = ‚àÇL/‚àÇz2 * h\n     ‚âà-0.14719 * 0.59874\n     ‚âà -0.08812\n     ‚àÇL/‚àÇb2 = ‚àÇL/‚àÇz2\n     ‚âà -0.14719\n\n6. Update Weights and Biases:\n   - Learning rate Œ± = 0.1.\n   - New weights and biases:\n     w1_new = w1 - Œ± * ‚àÇL/‚àÇw1\n     ‚âà 0.3 - 0.1 * (-0.00783)\n     ‚âà 0.300783\n     w2_new = w2 - Œ± * ‚àÇL/‚àÇw2\n     ‚âà 0.5 - 0.1 * 0 ‚âà 0.5\n     w3_new = w3 - Œ± * ‚àÇL/‚àÇw3\n     ‚âà 0.2 - 0.1 * (-0.08812) \n     ‚âà 0.208812\n     w4_new = w4 - Œ± * ‚àÇL/‚àÇw4 \n     ‚âà 0.4 - 0.1 * (-0.08812) \n     ‚âà 0.408812\n     b1_new = b1 - Œ± * ‚àÇL/‚àÇb1 \n     ‚âà 0.1 - 0.1 * (-0.01108) \n     ‚âà 0.100111\n     b2_new = b2 - Œ± * ‚àÇL/‚àÇb2 \n     ‚âà 0.6 - 0.1 * (-0.14719) \n     ‚âà 0.614719\n </pre>\n<br>\n<span>These updated values of weights and biases are used for forward propagation.</span><br>\n<span>Eventually, after many rounds of adjusting and fine-tuning, the neural network's weights and biases settle into values that help it make highly accurate predictions for the FM Rainbow channel (or any other task it was designed for). This process of finding the best weights and biases for the neural network is what we call \"training,\" and it allows the neural network to uncover hidden patterns and relationships in the data, much like how you discovered the FM Rainbow channel by tuning your radio.</span><br>"]
	},

	{
		"title": "Key Knows ",
		"logo": "https://img.freepik.com/premium-vector/key-logo-design-template-secure-symbol_18099-4057.jpg",
		"topic": "Key Knows ",
		"num": "2",
		"feeds": ["13-12-2023", "<span><span>KEY and Pretraining in TRANSFORMERS... üóù</span><br><b>QUERY:</b> I am the vector representing the word of interest.<br><b>KEY:</b> I am the vector holding the information on how much attention should be paid to each word with respect to QUERY.<br><b>VALUE:</b> I am the vector of the word of interest that gets weighted after knowing the contextual information of QUERY.<br><br><span>Here, how does KEY know how much attention should be paid to each word? If we have a sentence like 'The tiger comes from the south, and it is the direction of divinity', what does the word 'it' in the sentence refer to? Apparently, it refers to the direction 'south' because the tiger is not a direction! It's an animal belonging to the Panther Family (panthera tigris). üêØ</span><br><span>When QUERY is 'Tiger' and KEY is the word 'it', the multiplication of QUERY and KEY will have a low value. However, when QUERY is 'South' and KEY is the word 'it', the multiplication of QUERY and KEY will have a high value. ü§†</span><br><span>But how? How does KEY instruct the learning correctly? How did KEY acquire this information, something akin to human-level understanding? The answer is during Pretraining! üò≤</span><br><img src='https://raw.githubusercontent.com/arihara-sudhan/arihara-sudhan.github.io/main/statics/1701070443258.jpeg'><br><span>Pretraining is when the transformer learns how to determine the meaning of a word based on the CONTEXT! On a large chunk of data (corpus), pretraining is conducted to understand how words are aligned, how they are associated with each other, the underlying patterns, and so on. üòç</span><br><span><br>Yes, KEY information is learned during pretraining. ü§©</span><br><span>Stay connected and keep learning... ‚òòÔ∏è</span></span>"]
	},

	{
		"title": "Few Fewshots ",
		"logo": "https://img.freepik.com/premium-vector/key-logo-design-template-secure-symbol_18099-4057.jpg",
		"topic": "Few Fewshots ",
		"num": "3",
		"feeds": ["08-03-2024", "<span>
    <h4>(1/4) A FewShot Experiment : ResNet50 and ViT on High Intraclass Variance</h4>
    <span>The main goal here is to highlight the differences in performance between ResNet50 and the ViT model when dealing with data characterized by high intraclass variance, while noting their strengths in simpler datasets like MNIST and Omniglot. üëæ 
In simple datasets such as MNIST and Omniglot, both models performed exceptionally well. Specifically, ResNet50 achieved an impressive accuracy rate of 99%, showcasing its prowess in capturing patterns effectively. In contrast, the ViT Model achieved a slightly lower accuracy of 96%. This outcome is understandable, as ResNet50 excels at pattern recognition in these low-intraclass variance datasets. ü¶â 
    
    However, when more complex data with high intraclass variance was introduced, the performance gap between the two models became more apparent. ResNet50's accuracy dropped significantly to 56%, demonstrating its struggle with the increased intraclass variance. On the other hand, the ViT Model's accuracy remained relatively higher at 77%, indicating its ability to comprehend the context and adapt to datasets with more variability. üìâ 
    <img src="https://github.com/arihara-sudhan/arihara-sudhan.github.io/blob/main/statics/blog1.jpeg?raw=true" alt="imghere"><br>
    It's important to emphasize that both ResNet50 and the ViT Model have their unique strengths and weaknesses. A ViT Model may not be capable of accomplishing what a ResNet50 can do, and vice versa. Each model is optimized for specific tasks and scenarios, and their performance varies based on the nature of the data they are presented with. ‚ù§Ô∏è
    
    <br><a href="https://lnkd.in/gk9sf_hc">Link to REPO</a>
    
</span>
<br>
<hr>

<span>
    <h4>(2/4) FewShot Model tends to behave like A Classification Model??? üò≤</h4>
    <span>
        <img src="https://github.com/arihara-sudhan/arihara-sudhan.github.io/blob/main/statics/blog2.jpeg?raw=true" alt="img"><br>
        So, what I've been experimenting with is a FewShot model, and it turns out that its generalization depends on the number of unseen classes during testing. I initiated the experiment by training the model using a triplet setup with the MNIST images belonging to category 7, 8, and 9. üçÄ 

When I trained the model on just those three classes (7, 8, 9), it performed remarkably well, achieving an accuracy of 98%. However, as I introduced more unseen classes during testing, things got a bit trickier. Testing on two new classes (5, 6) still gave me a respectable accuracy of 90%. A Considerable Generalization! üôÇ 

As I expanded the testing set to include three unseen classes (4, 5, 6), the accuracy dropped slightly to 89%. It continued to drop as I included four unseen classes (3, 4, 5, 6), reaching 84%. The model started to struggle with entirely different classes. But then, something interesting happened when I introduced class 1. Its similarity to class 7 helped the model, and the accuracy went up. üòÉ 

When testing with six classes (1, 2, 3, 4, 5, 6), the accuracy was 81%. However, when I replaced class 1 with class 0, I noticed a drop in accuracy. 
Testing with these six classes (0, 2, 3, 4, 5, 6) gave me an accuracy of 77%. Six Unseen classes together (0, 1, 2, 3, 4, 5, 6) brought the accuracy back up to 80%, but it still wasn't as high as when I started with just three classes (7, 8, 9). üôÉ 

It appears that the model's generalization performance is influenced by the number of classes used during training. Introducing a greater number of classes during testing than what was used for training tends to confuse the model. Let's consider training with a larger number of classes and, during testing, aim to keep the number of classes lower than those used in training to an extent! ‚úç 
<br>
<br>
Trained on 3 (7,8,9): 98%
<br>
Testing on 2 (5,6) : 90%  [K-MEANS : 89%] 
<br>
Testing on 3 (4,5,6) : 89%  [K-MEANS : 79%] 
<br>
Testing on 4 (3,4,5,6) : 84%  [K-MEANS : 70%] 
<br>
Testing on 5 (2,3,4,5,6) : 79%  [K-MEANS : 61%] 
<br>
Testing on 6 (1,2,3,4,5,6) : 81%  [K-MEANS : 64%]
<br>
Testing on 6 (0,2,3,4,5,6) : 77%  [K-MEANS : 55%]
<br>
Testing on 7 (0,1,2,3,4,5,6) : 80%  [K-MEANS : 63%] 
<br>
<br>
    </span><br>
    <span>
        Doubts may arise regarding whether a low number of classes leads to fewer mispredictions. This example illustrates the behavior of a few-shot model, which doesn't fall within our assumptions...
        <img src="https://github.com/arihara-sudhan/arihara-sudhan.github.io/blob/main/statics/blog3.png?raw=true" alt="img"><br>
    </span>
</span>
<br>
<hr>

<span>
    <h4>(3/4) Classification Model tends to behave like A FewShot Model??? üò≤</h4>
While a classifier model may not excel in precisely classifying unseen classes (Completely Different Classes), it can demonstrate the ability to create meaningful clusters for unknown classes.

I trained a classifier on datasets containing classes of various shades of red, green, and blue. The model efficiently grouped these colors into clusters resembling Starlink satellites. ‚≠êÔ∏è

Subsequently, when I introduced classes featuring shades of orange, pink, and violet during testing , the model displayed a distinctive cluster formation. Although not as potent as specialized few-shot techniques like Triplet or Contrastive Training, which are explicitly designed for such scenarios, this classifier performed admirably in identifying underlying patterns, particularly when dealing with simple image categories such as color shades.
<img src="https://github.com/arihara-sudhan/arihara-sudhan.github.io/blob/main/statics/blog4.jpeg?raw=true" alt="img"><br>
<br>
Interesting! üòÉ
</span>
<hr>
<span>
    <h4>(4/4)üí° IDEA : Collaged Images as Embeddings in FewShot Learning!</h4>

In Few-Shot Learning, we conduct training to make the embeddings of similar samples closer to each other and dissimilar samples farther apart. During testing, we need to extract embeddings for the samples by inputting them into the trained model. We require a database of embeddings where distinct clusters for each class result from our training. When testing with samples, we search for the nearest embeddings of those samples and calculate accuracy based on this. (We have one grape fruit and search to which grape-cluster it belongs to üçá)

This approach becomes resource-intensive when dealing with a large number of samples that need to be stored as embeddings. On the other hand, we cannot ignore any samples, as each contributes to achieving good results. If we simply follow this approach, it becomes time-consuming to extract embeddings for all the samples, and memory usage also increases. üíÄ

I thought like: why not create collages of sample images, extract their embeddings, and store them in the database? This would reduce the space and time required, depending on the number of samples we need in a single image. I conducted experiments with a simple dataset and achieved 99% accuracy using the conventional method with individual samples. However, it was resource-intensive. When I introduced collages (3x3) of samples, we reduced the number of samples, the time required for embedding extraction, and memory usage. The accuracy was still 99%! üòç

<img src="https://github.com/arihara-sudhan/arihara-sudhan.github.io/blob/main/statics/blog5.jpeg?raw=true" alt="img"><br>
Furthermore, we can observe that one embedding collectively represents the features of multiple images. This way, misclustering is reduced to some extent when considering them collectively (Outliers Problem Solved ü§≠).
<br>
<br>
<h6>NON-COLLAGED EMBEDDINGS</h6>TOTAL SAMPLES : 1621 | EMBEDDINGS EXTRACTION TIME : 35 SECONDS | SIZE OF EMBEDDINGS : 981 MB | ACCURACY : 99.01 %
 <br>

<h6>COLLAGED EMBEDDINGS</h6>
TOTAL SAMPLES : 182 | EMBEDDINGS EXTRACTION TIME : 3 SECONDS | SIZE OF EMBEDDINGS : 105 MB | ACCURACY : 99.01 %
 <br>"]
	}
]
